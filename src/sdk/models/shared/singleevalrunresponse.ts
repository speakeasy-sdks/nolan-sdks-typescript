/*
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

import { objectToClass, SpeakeasyBase, SpeakeasyMetadata } from "../../../internal/utils";
import { LogEntry } from "./logentry";
import { PipelineNodeConfig } from "./pipelinenodeconfig";
import { Tag } from "./tag";
import { Expose, Transform, Type } from "class-transformer";

/**
 * The user who created the eval run.
 */
export class SingleEvalRunResponseOauthUser extends SpeakeasyBase {
    /**
     * Family name of a user
     */
    @SpeakeasyMetadata()
    @Expose({ name: "family_name" })
    familyName: string;

    /**
     * Given name of a user
     */
    @SpeakeasyMetadata()
    @Expose({ name: "given_name" })
    givenName: string;

    @SpeakeasyMetadata()
    @Expose({ name: "user_id" })
    userId: string;
}

/**
 * Parameters set for this evaluation run
 */
export class SingleEvalRunResponseEvalRunParameters extends SpeakeasyBase {
    /**
     * Turns the debug mode on for this evaluation run. The debug mode shows you what went wrong if the evaluation run fails.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "debug" })
    debug: boolean;

    /**
     * A unique identifier of the evaluation set used for the evaluation run.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "evaluation_set_id" })
    evaluationSetId: string;

    /**
     * The name of the evaluation set used for the evaluation run.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "evaluation_set_name" })
    evaluationSetName: string;

    /**
     * The name of the pipeline used for evaluation.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "pipeline_name" })
    pipelineName: string;

    /**
     * The date and time when the pipeline snapshot was taken.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "pipeline_snapshot_at" })
    @Transform(({ value }) => new Date(value), { toClassOnly: true })
    pipelineSnapshotAt: Date;

    /**
     * Pipeline YAML at the time of the snapshot.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "pipeline_snapshot_yaml" })
    pipelineSnapshotYaml: string;

    /**
     * The name of the model used to calculate SAS metrics during an experiment.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "sas_model_name" })
    sasModelName?: string;
}

/**
 * The metrics for the whole pipeline.
 */
export class SingleEvalRunResponsePipelineMetric extends SpeakeasyBase {
    /**
     * The number of exact matches of the pipeline. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "integrated_exact_match" })
    integratedExactMatch?: number;

    /**
     * The F1 score of the pipeline. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "integrated_f1" })
    integratedF1?: number;

    /**
     * The mean average precision of the pipeline. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "integrated_mean_average_precision" })
    integratedMeanAveragePrecision?: number;

    /**
     * The mean reciprocal rank of the pipeline. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "integrated_mean_reciprocal_rank" })
    integratedMeanReciprocalRank?: number;

    /**
     * The normal discounted cumulative gain of the pipeline. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "integrated_normal_discounted_cumulative_gain" })
    integratedNormalDiscountedCumulativeGain?: number;

    /**
     * The precision of the pipeline. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "integrated_precision" })
    integratedPrecision?: number;

    /**
     * The recall multi hit metric of the pipeline. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "integrated_recall_multi_hit" })
    integratedRecallMultiHit?: number;

    /**
     * The recall single hit metric of the pipeline. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "integrated_recall_single_hit" })
    integratedRecallSingleHit?: number;

    /**
     * The SAS score of the pipeline. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "integrated_sas" })
    integratedSas?: number;

    /**
     * The number of exact matches of the last answer_node in isolated mode. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "isolated_exact_match" })
    isolatedExactMatch?: number;

    /**
     * The F1 score of the last answer_node in isolated mode. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "isolated_f1" })
    isolatedF1?: number;

    /**
     * The SAS score of the last answer_node in isolated mode. For more information, see [Experiments and Metrics](https://docs.cloud.deepset.ai/docs/experiments-and-metrics)
     */
    @SpeakeasyMetadata()
    @Expose({ name: "isolated_sas" })
    isolatedSas?: number;
}

/**
 * Successful Response
 */
export class SingleEvalRunResponse extends SpeakeasyBase {
    /**
     * Add a comment about this evaluation run.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "comment" })
    comment?: string;

    /**
     * The date and time when the evaluation run was created.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "created_at" })
    @Transform(({ value }) => new Date(value), { toClassOnly: true })
    createdAt: Date;

    /**
     * The user who created the eval run.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "created_by" })
    @Type(() => SingleEvalRunResponseOauthUser)
    createdBy: SingleEvalRunResponseOauthUser;

    /**
     * Contains the evaluated pipeline nodes and their overall metrics.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "eval_results" })
    evalResults?: any[];

    /**
     * A unique identifier of the evaluation run.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "eval_run_id" })
    evalRunId: string;

    /**
     * The date and time when the evaluation run was last edited.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "last_edited_at" })
    @Transform(({ value }) => new Date(value), { toClassOnly: true })
    lastEditedAt?: Date;

    /**
     * The user who created the eval run.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "last_edited_by" })
    @Type(() => SingleEvalRunResponseOauthUser)
    lastEditedBy?: SingleEvalRunResponseOauthUser;

    /**
     * Contains the logs of the evaluation run.
     */
    @SpeakeasyMetadata({ elemType: LogEntry })
    @Expose({ name: "logs" })
    @Type(() => LogEntry)
    logs: LogEntry[];

    /**
     * Unique name of an evaluation run.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "name" })
    name: string;

    /**
     * Parameters set for this evaluation run
     */
    @SpeakeasyMetadata()
    @Expose({ name: "parameters" })
    @Type(() => SingleEvalRunResponseEvalRunParameters)
    parameters: SingleEvalRunResponseEvalRunParameters;

    /**
     * The metrics for the whole pipeline.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "pipeline_metrics" })
    @Type(() => SingleEvalRunResponsePipelineMetric)
    pipelineMetrics: SingleEvalRunResponsePipelineMetric;

    /**
     * The parameters for each pipeline node with key and value.
     */
    @SpeakeasyMetadata({ elemType: PipelineNodeConfig })
    @Expose({ name: "pipeline_parameters" })
    @Transform(
        ({ value }) => {
            const obj: Record<string, PipelineNodeConfig> = {};
            for (const key in value) {
                obj[key] = objectToClass(value[key], PipelineNodeConfig);
            }
            return obj;
        },
        { toClassOnly: true }
    )
    pipelineParameters: Record<string, PipelineNodeConfig>;

    /**
     * Status of the evaluation run. Returns one of these values: CREATED, STARTED, FAILED, ENDED.
     */
    @SpeakeasyMetadata()
    @Expose({ name: "status" })
    status: string;

    /**
     * A list of tags associated with the evaluation run.
     */
    @SpeakeasyMetadata({ elemType: Tag })
    @Expose({ name: "tags" })
    @Type(() => Tag)
    tags: Tag[];
}
